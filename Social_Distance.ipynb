{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Object_Detection_Distance.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kFSqkTCdWKMI"},"source":["# Imports"]},{"cell_type":"markdown","metadata":{"id":"q6Xi9UJKRyz7"},"source":["We have to set the tensorflow version using `tensorflow_version 1.x`"]},{"cell_type":"code","metadata":{"id":"XXkAE-6_LrJN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616325540920,"user_tz":-300,"elapsed":6413,"user":{"displayName":"AhmedShahid Muhammadi","photoUrl":"","userId":"11627724979041663556"}},"outputId":"bf226a3c-a508-47ed-9a75-f299ff03503c"},"source":["%tensorflow_version 1.x\n","\n","import tensorflow as tf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ESovLlfZPabA","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"16fa59b3-d483-46bd-93e8-072f2a83aac6"},"source":["!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","!pip install -qq Cython contextlib2 pillow lxml matplotlib pycocotools\n","!pip install tf_slim\n","!pip install numpy==1.17.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package python-bs4.\n","(Reading database ... 160980 files and directories currently installed.)\n","Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n","Unpacking python-bs4 (4.6.0-1) ...\n","Selecting previously unselected package python-pkg-resources.\n","Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n","Unpacking python-pkg-resources (39.0.1-2) ...\n","Selecting previously unselected package python-chardet.\n","Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n","Unpacking python-chardet (3.0.4-1) ...\n","Selecting previously unselected package python-six.\n","Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n","Unpacking python-six (1.11.0-2) ...\n","Selecting previously unselected package python-webencodings.\n","Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n","Unpacking python-webencodings (0.5-2) ...\n","Selecting previously unselected package python-html5lib.\n","Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n","Unpacking python-html5lib (0.999999999-1) ...\n","Selecting previously unselected package python-lxml:amd64.\n","Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.3_amd64.deb ...\n","Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.3) ...\n","Selecting previously unselected package python-olefile.\n","Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n","Unpacking python-olefile (0.45.1-1) ...\n","Selecting previously unselected package python-pil:amd64.\n","Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.5_amd64.deb ...\n","Unpacking python-pil:amd64 (5.1.0-1ubuntu0.5) ...\n","Setting up python-pkg-resources (39.0.1-2) ...\n","Setting up python-six (1.11.0-2) ...\n","Setting up python-bs4 (4.6.0-1) ...\n","Setting up python-lxml:amd64 (4.2.1-1ubuntu0.3) ...\n","Setting up python-olefile (0.45.1-1) ...\n","Setting up python-pil:amd64 (5.1.0-1ubuntu0.5) ...\n","Setting up python-webencodings (0.5-2) ...\n","Setting up python-chardet (3.0.4-1) ...\n","Setting up python-html5lib (0.999999999-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting tf_slim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n","\u001b[K     |████████████████████████████████| 358kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n","Collecting numpy==1.17.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/4b/55cfbfd3e5e85016eeef9f21c0ec809d978706a0d60b62cc28aeec8c792f/numpy-1.17.0-cp37-cp37m-manylinux1_x86_64.whl (20.3MB)\n","\u001b[K     |████████████████████████████████| 20.3MB 76.5MB/s \n","\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","Successfully installed numpy-1.17.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"1VcJ91l2OR7F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616326021658,"user_tz":-300,"elapsed":1094,"user":{"displayName":"AhmedShahid Muhammadi","photoUrl":"","userId":"11627724979041663556"}},"outputId":"fab30c83-9200-44e0-f4ce-54c8c67c97b7"},"source":["%cd /models/research"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/MyDrive/Projects/Pedestrian_Detection/models/research\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIRmZpQQZQeu","outputId":"d9ff5c4d-4921-4d0d-f9a5-2bffc9b80cca"},"source":["# !pwd\n","# !protoc object_detection/protos/*.proto --python_out=."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Projects/Pedestrian_Detection/models/research\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UxNGvgf0OdTr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616326108821,"user_tz":-300,"elapsed":77127,"user":{"displayName":"AhmedShahid Muhammadi","photoUrl":"","userId":"11627724979041663556"}},"outputId":"c36e4670-f1f4-4040-d20c-8e1db9800573"},"source":["%cd /models/research/slim/\n","\n","import os\n","os.environ['PYTHONPATH'] += '/content/drive/MyDrive/Projects/Pedestrian_Detection/models/research:/content/drive/MyDrive/Projects/Pedestrian_Detection/models/research/slim/'\n","\n","!python setup.py build\n","!python setup.py install"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/MyDrive/Projects/Pedestrian_Detection/models/research/slim\n","running build\n","running build_py\n","running egg_info\n","writing slim.egg-info/PKG-INFO\n","writing dependency_links to slim.egg-info/dependency_links.txt\n","writing requirements to slim.egg-info/requires.txt\n","writing top-level names to slim.egg-info/top_level.txt\n","writing manifest file 'slim.egg-info/SOURCES.txt'\n","running install\n","running bdist_egg\n","running egg_info\n","writing slim.egg-info/PKG-INFO\n","writing dependency_links to slim.egg-info/dependency_links.txt\n","writing requirements to slim.egg-info/requires.txt\n","writing top-level names to slim.egg-info/top_level.txt\n","writing manifest file 'slim.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet_2012_validation_synset_labels.txt -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/preprocess_imagenet_validation_data.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_flowers.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/flowers.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_imagenet.sh -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/dataset_utils.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_imagenet.sh -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/mnist.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_mnist.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/visualwakewords.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/build_imagenet_data.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_cifar10.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/dataset_factory.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_visualwakewords.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/cifar10.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet_metadata.txt -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/process_bounding_boxes.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/download_and_convert_visualwakewords_lib.py -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/imagenet_lsvrc_2015_synsets.txt -> build/bdist.linux-x86_64/egg/datasets\n","copying build/lib/datasets/__init__.py -> build/bdist.linux-x86_64/egg/datasets\n","creating build/bdist.linux-x86_64/egg/deployment\n","copying build/lib/deployment/__init__.py -> build/bdist.linux-x86_64/egg/deployment\n","copying build/lib/deployment/model_deploy.py -> build/bdist.linux-x86_64/egg/deployment\n","copying build/lib/deployment/model_deploy_test.py -> build/bdist.linux-x86_64/egg/deployment\n","creating build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v2.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/cifarnet.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v4.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/cyclegan.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/dcgan.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/i3d_utils.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1.png -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/overfeat.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/nets_factory.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/cyclegan_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/overfeat_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1.md -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v3_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/alexnet.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/alexnet_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/i3d_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/lenet.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v4_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v2_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_utils.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1_train.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/__init__.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_resnet_v2_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_resnet_v2.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v1.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/dcgan_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v3.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/i3d.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/inception_v1_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/nets_factory_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/mobilenet_v1_eval.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v1_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/vgg.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v2_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_utils.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/vgg_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/pix2pix.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v1.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/post_training_quantization.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/resnet_v2.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/s3dg_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/pix2pix_test.py -> build/bdist.linux-x86_64/egg/nets\n","copying build/lib/nets/s3dg.py -> build/bdist.linux-x86_64/egg/nets\n","creating build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/conv_blocks.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_v3.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mnet_v1_vs_v2_pixel1_latency.png -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_v2.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/README.md -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_v3_test.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/__init__.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_v2_test.py -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","copying build/lib/nets/mobilenet/mobilenet_example.ipynb -> build/bdist.linux-x86_64/egg/nets/mobilenet\n","creating build/bdist.linux-x86_64/egg/nets/mobilenet/g3doc\n","copying build/lib/nets/mobilenet/g3doc/latency_pixel1.png -> build/bdist.linux-x86_64/egg/nets/mobilenet/g3doc\n","copying build/lib/nets/mobilenet/g3doc/madds_top1_accuracy.png -> build/bdist.linux-x86_64/egg/nets/mobilenet/g3doc\n","copying build/lib/nets/mobilenet/g3doc/edgetpu_latency.png -> build/bdist.linux-x86_64/egg/nets/mobilenet/g3doc\n","creating build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/pnasnet.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/pnasnet_test.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet_test.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/README.md -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet_utils.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet_utils_test.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/__init__.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","copying build/lib/nets/nasnet/nasnet.py -> build/bdist.linux-x86_64/egg/nets/nasnet\n","creating build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/__init__.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/preprocessing_factory.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/vgg_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/lenet_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/cifarnet_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","copying build/lib/preprocessing/inception_preprocessing.py -> build/bdist.linux-x86_64/egg/preprocessing\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/imagenet.py to imagenet.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/preprocess_imagenet_validation_data.py to preprocess_imagenet_validation_data.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_flowers.py to download_and_convert_flowers.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/flowers.py to flowers.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/dataset_utils.py to dataset_utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/mnist.py to mnist.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_mnist.py to download_and_convert_mnist.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/visualwakewords.py to visualwakewords.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/build_imagenet_data.py to build_imagenet_data.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_cifar10.py to download_and_convert_cifar10.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/dataset_factory.py to dataset_factory.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_visualwakewords.py to download_and_convert_visualwakewords.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/cifar10.py to cifar10.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/process_bounding_boxes.py to process_bounding_boxes.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/download_and_convert_visualwakewords_lib.py to download_and_convert_visualwakewords_lib.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/datasets/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/deployment/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/deployment/model_deploy.py to model_deploy.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/deployment/model_deploy_test.py to model_deploy_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v2.py to inception_v2.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/cifarnet.py to cifarnet.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v4.py to inception_v4.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1.py to mobilenet_v1.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/cyclegan.py to cyclegan.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/dcgan.py to dcgan.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/i3d_utils.py to i3d_utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1_test.py to mobilenet_v1_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/overfeat.py to overfeat.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nets_factory.py to nets_factory.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/cyclegan_test.py to cyclegan_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/overfeat_test.py to overfeat_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v3_test.py to inception_v3_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/alexnet.py to alexnet.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/alexnet_test.py to alexnet_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/i3d_test.py to i3d_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/lenet.py to lenet.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v4_test.py to inception_v4_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v2_test.py to inception_v2_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_utils.py to inception_utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1_train.py to mobilenet_v1_train.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception.py to inception.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_resnet_v2_test.py to inception_resnet_v2_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_resnet_v2.py to inception_resnet_v2.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v1.py to inception_v1.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/dcgan_test.py to dcgan_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v3.py to inception_v3.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/i3d.py to i3d.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/inception_v1_test.py to inception_v1_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nets_factory_test.py to nets_factory_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet_v1_eval.py to mobilenet_v1_eval.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v1_test.py to resnet_v1_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/vgg.py to vgg.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v2_test.py to resnet_v2_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_utils.py to resnet_utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/vgg_test.py to vgg_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/pix2pix.py to pix2pix.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v1.py to resnet_v1.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/post_training_quantization.py to post_training_quantization.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/resnet_v2.py to resnet_v2.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/s3dg_test.py to s3dg_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/pix2pix_test.py to pix2pix_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/s3dg.py to s3dg.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/conv_blocks.py to conv_blocks.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet_v3.py to mobilenet_v3.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet_v2.py to mobilenet_v2.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet_v3_test.py to mobilenet_v3_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet.py to mobilenet.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/mobilenet/mobilenet_v2_test.py to mobilenet_v2_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/pnasnet.py to pnasnet.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/pnasnet_test.py to pnasnet_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet_test.py to nasnet_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet_utils.py to nasnet_utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet_utils_test.py to nasnet_utils_test.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/nets/nasnet/nasnet.py to nasnet.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/preprocessing_factory.py to preprocessing_factory.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/vgg_preprocessing.py to vgg_preprocessing.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/lenet_preprocessing.py to lenet_preprocessing.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/cifarnet_preprocessing.py to cifarnet_preprocessing.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/preprocessing/inception_preprocessing.py to inception_preprocessing.cpython-37.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying slim.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating 'dist/slim-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing slim-0.1-py3.7.egg\n","Copying slim-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n","Adding slim 0.1 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.7/dist-packages/slim-0.1-py3.7.egg\n","Processing dependencies for slim==0.1\n","Searching for tf-slim>=1.1\n","Reading https://pypi.org/simple/tf-slim/\n","Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl#sha256=fa2bab63b3925bd42601102e7f178dce997f525742596bf404fa8a6918e146ff\n","Best match: tf-slim 1.1.0\n","Processing tf_slim-1.1.0-py2.py3-none-any.whl\n","Installing tf_slim-1.1.0-py2.py3-none-any.whl to /usr/local/lib/python3.7/dist-packages\n","Adding tf-slim 1.1.0 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.7/dist-packages/tf_slim-1.1.0-py3.7.egg\n","Searching for six==1.15.0\n","Best match: six 1.15.0\n","Adding six 1.15.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for absl-py==0.10.0\n","Best match: absl-py 0.10.0\n","Adding absl-py 0.10.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Finished processing dependencies for slim==0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wy72mWwAWKMK"},"source":["## Importing Libraries"]},{"cell_type":"code","metadata":{"id":"hV4P5gyTWKMI"},"source":["import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","from distutils.version import StrictVersion\n","from collections import defaultdict\n","from io import StringIO\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","# This is needed since the notebook is stored in the object_detection folder.\n","sys.path.append(\"..\")\n","#from object_detection.utils import ops as utils_ops\n","\n","if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n","  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7m_NY_aWKMK"},"source":["# This is needed to display the images.\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5FNuiRPWKMN"},"source":["## Object detection imports\n","Here are the imports from the object detection module."]},{"cell_type":"code","metadata":{"id":"bm0_uNRnWKMN"},"source":["from object_detection.utils import label_map_util\n","\n","from object_detection.utils import visualization_utils as vis_util\n","\n","import math\n","import itertools\n","from itertools import compress\n","from PIL import Image, ImageDraw"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cfn_tRFOWKMO"},"source":["# Model preparation "]},{"cell_type":"markdown","metadata":{"id":"X_sEBLpVWKMQ"},"source":["## Variables\n","\n","Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n","\n","By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."]},{"cell_type":"code","metadata":{"id":"xKWKG2oefn0X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616326494859,"user_tz":-300,"elapsed":1107,"user":{"displayName":"AhmedShahid Muhammadi","photoUrl":"","userId":"11627724979041663556"}},"outputId":"ae30fa6e-2f32-42fe-db99-83fe2b2b92a2"},"source":["#!mkdir /content/drive/MyDrive/Projects/Pedestrian_Detection/models/research/pretrained_model\n","%cd /models/research/pretrained_model\n","\n","# !wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n","# !tar -xzf ssd_mobilenet_v2_coco_2018_03_29.tar.gz -C .\n","\n","%cd models/research"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/MyDrive/Projects/Pedestrian_Detection/models/research/pretrained_model\n","/gdrive/MyDrive/Projects/Pedestrian_Detection/models/research\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VyPz_t8WWKMQ"},"source":["# Model Name\n","MODEL_NAME = 'ssd_mobilenet_v2_coco_2018_03_29'\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_FROZEN_GRAPH = '/models/research/pretrained_model/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb'\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS = '/models/research/object_detection/data/mscoco_label_map.pbtxt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YBcB9QHLWKMU"},"source":["## Load a (frozen) Tensorflow model into memory."]},{"cell_type":"code","metadata":{"id":"KezjCRVvWKMV"},"source":["detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","  od_graph_def = tf.GraphDef()\n","  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n","    serialized_graph = fid.read()\n","    od_graph_def.ParseFromString(serialized_graph)\n","    tf.import_graph_def(od_graph_def, name='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1MVVTcLWKMW"},"source":["## Loading label map\n","Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"]},{"cell_type":"code","metadata":{"id":"hDbpHkiWWKMX"},"source":["category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EFsoUHvbWKMZ"},"source":["## Helper code"]},{"cell_type":"code","metadata":{"id":"aSlYc3JkWKMa"},"source":["def load_image_into_numpy_array(image):\n","  (im_width, im_height) = image.size\n","  return np.array(image.getdata()).reshape(\n","      (im_height, im_width, 3)).astype(np.uint8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0_1AGhrWKMc"},"source":["# Detection"]},{"cell_type":"code","metadata":{"id":"jG-zn5ykWKMd"},"source":["# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n","PATH_TO_TEST_IMAGES_DIR = '/models/research/object_detection/test_images'\n","#TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n","\n","# Size, in inches, of the output images.\n","IMAGE_SIZE = (12, 8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92BHxzcNWKMf"},"source":["def run_inference_for_single_image(image, graph):\n","  with graph.as_default():\n","    with tf.Session() as sess:\n","      # Get handles to input and output tensors\n","      ops = tf.get_default_graph().get_operations()\n","      all_tensor_names = {output.name for op in ops for output in op.outputs}\n","      tensor_dict = {}\n","      for key in [\n","          'num_detections', 'detection_boxes', 'detection_scores',\n","          'detection_classes', 'detection_masks'\n","      ]:\n","        tensor_name = key + ':0'\n","        if tensor_name in all_tensor_names:\n","          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","              tensor_name)\n","      if 'detection_masks' in tensor_dict:\n","        # The following processing is only for single image\n","        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n","        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n","        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n","        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n","        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n","        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","        detection_masks_reframed = tf.cast(\n","            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","        # Follow the convention by adding back the batch dimension\n","        tensor_dict['detection_masks'] = tf.expand_dims(\n","            detection_masks_reframed, 0)\n","      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","      # Run inference\n","      output_dict = sess.run(tensor_dict,\n","                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","      # all outputs are float32 numpy arrays, so convert types as appropriate\n","      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n","      output_dict['detection_classes'] = output_dict[\n","          'detection_classes'][0].astype(np.uint8)\n","      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","      if 'detection_masks' in output_dict:\n","        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","  return output_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yt5NysTCXLAP"},"source":["Function to filter predictions by label. We only want display `person`, in this case `person` is `id=1`. We can check it in `category_index`"]},{"cell_type":"code","metadata":{"id":"4ploMTJ8XfgH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616326708907,"user_tz":-300,"elapsed":1138,"user":{"displayName":"AhmedShahid Muhammadi","photoUrl":"","userId":"11627724979041663556"}},"outputId":"2315b2b4-ca2b-40fc-a350-0799941dc573"},"source":["category_index"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{1: {'id': 1, 'name': 'person'},\n"," 2: {'id': 2, 'name': 'bicycle'},\n"," 3: {'id': 3, 'name': 'car'},\n"," 4: {'id': 4, 'name': 'motorcycle'},\n"," 5: {'id': 5, 'name': 'airplane'},\n"," 6: {'id': 6, 'name': 'bus'},\n"," 7: {'id': 7, 'name': 'train'},\n"," 8: {'id': 8, 'name': 'truck'},\n"," 9: {'id': 9, 'name': 'boat'},\n"," 10: {'id': 10, 'name': 'traffic light'},\n"," 11: {'id': 11, 'name': 'fire hydrant'},\n"," 13: {'id': 13, 'name': 'stop sign'},\n"," 14: {'id': 14, 'name': 'parking meter'},\n"," 15: {'id': 15, 'name': 'bench'},\n"," 16: {'id': 16, 'name': 'bird'},\n"," 17: {'id': 17, 'name': 'cat'},\n"," 18: {'id': 18, 'name': 'dog'},\n"," 19: {'id': 19, 'name': 'horse'},\n"," 20: {'id': 20, 'name': 'sheep'},\n"," 21: {'id': 21, 'name': 'cow'},\n"," 22: {'id': 22, 'name': 'elephant'},\n"," 23: {'id': 23, 'name': 'bear'},\n"," 24: {'id': 24, 'name': 'zebra'},\n"," 25: {'id': 25, 'name': 'giraffe'},\n"," 27: {'id': 27, 'name': 'backpack'},\n"," 28: {'id': 28, 'name': 'umbrella'},\n"," 31: {'id': 31, 'name': 'handbag'},\n"," 32: {'id': 32, 'name': 'tie'},\n"," 33: {'id': 33, 'name': 'suitcase'},\n"," 34: {'id': 34, 'name': 'frisbee'},\n"," 35: {'id': 35, 'name': 'skis'},\n"," 36: {'id': 36, 'name': 'snowboard'},\n"," 37: {'id': 37, 'name': 'sports ball'},\n"," 38: {'id': 38, 'name': 'kite'},\n"," 39: {'id': 39, 'name': 'baseball bat'},\n"," 40: {'id': 40, 'name': 'baseball glove'},\n"," 41: {'id': 41, 'name': 'skateboard'},\n"," 42: {'id': 42, 'name': 'surfboard'},\n"," 43: {'id': 43, 'name': 'tennis racket'},\n"," 44: {'id': 44, 'name': 'bottle'},\n"," 46: {'id': 46, 'name': 'wine glass'},\n"," 47: {'id': 47, 'name': 'cup'},\n"," 48: {'id': 48, 'name': 'fork'},\n"," 49: {'id': 49, 'name': 'knife'},\n"," 50: {'id': 50, 'name': 'spoon'},\n"," 51: {'id': 51, 'name': 'bowl'},\n"," 52: {'id': 52, 'name': 'banana'},\n"," 53: {'id': 53, 'name': 'apple'},\n"," 54: {'id': 54, 'name': 'sandwich'},\n"," 55: {'id': 55, 'name': 'orange'},\n"," 56: {'id': 56, 'name': 'broccoli'},\n"," 57: {'id': 57, 'name': 'carrot'},\n"," 58: {'id': 58, 'name': 'hot dog'},\n"," 59: {'id': 59, 'name': 'pizza'},\n"," 60: {'id': 60, 'name': 'donut'},\n"," 61: {'id': 61, 'name': 'cake'},\n"," 62: {'id': 62, 'name': 'chair'},\n"," 63: {'id': 63, 'name': 'couch'},\n"," 64: {'id': 64, 'name': 'potted plant'},\n"," 65: {'id': 65, 'name': 'bed'},\n"," 67: {'id': 67, 'name': 'dining table'},\n"," 70: {'id': 70, 'name': 'toilet'},\n"," 72: {'id': 72, 'name': 'tv'},\n"," 73: {'id': 73, 'name': 'laptop'},\n"," 74: {'id': 74, 'name': 'mouse'},\n"," 75: {'id': 75, 'name': 'remote'},\n"," 76: {'id': 76, 'name': 'keyboard'},\n"," 77: {'id': 77, 'name': 'cell phone'},\n"," 78: {'id': 78, 'name': 'microwave'},\n"," 79: {'id': 79, 'name': 'oven'},\n"," 80: {'id': 80, 'name': 'toaster'},\n"," 81: {'id': 81, 'name': 'sink'},\n"," 82: {'id': 82, 'name': 'refrigerator'},\n"," 84: {'id': 84, 'name': 'book'},\n"," 85: {'id': 85, 'name': 'clock'},\n"," 86: {'id': 86, 'name': 'vase'},\n"," 87: {'id': 87, 'name': 'scissors'},\n"," 88: {'id': 88, 'name': 'teddy bear'},\n"," 89: {'id': 89, 'name': 'hair drier'},\n"," 90: {'id': 90, 'name': 'toothbrush'}}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"_5X-o1xZhbp_"},"source":["def filter_boxes(min_score, boxes, scores, classes, categories):\n","  \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n","  n = len(classes)\n","  idxs = []\n","  for i in range(n):\n","      if classes[i] in categories and scores[i] >= min_score:\n","          idxs.append(i)\n","  \n","  filtered_boxes = boxes[idxs, ...]\n","  filtered_scores = scores[idxs, ...]\n","  filtered_classes = classes[idxs, ...]\n","  return filtered_boxes, filtered_scores, filtered_classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-j8SK-8aFmY"},"source":["When we get the prediction from Tensorflow, we get a list of boxes with all predictions. This function allows us to get the coordinates of a box."]},{"cell_type":"code","metadata":{"id":"k3Ah5pJib7lM"},"source":["def calculate_coord(bbox, width, height):\n","    xmin = bbox[1] * width\n","    ymin = bbox[0] * height\n","    xmax = bbox[3] * width\n","    ymax = bbox[2] * height\n","\n","    return [xmin, ymin, xmax - xmin, ymax - ymin]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLdxjRdYaZ2N"},"source":["Function to calculate the centroid"]},{"cell_type":"code","metadata":{"id":"zSbobR-Rhmpv"},"source":["def calculate_centr(coord):\n","  return (coord[0]+(coord[2]/2), coord[1]+(coord[3]/2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0Aas60Jaar8"},"source":["Calculate distance between 2 centroids"]},{"cell_type":"code","metadata":{"id":"dFTU801Uu_6G"},"source":["def calculate_centr_distances(centroid_1, centroid_2):\n","  return  math.sqrt((centroid_2[0]-centroid_1[0])**2 + (centroid_2[1]-centroid_1[1])**2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fT9tbXPeaed6"},"source":["Calculate all permutations between the centroids"]},{"cell_type":"code","metadata":{"id":"_pE1VVF_54my"},"source":["def calculate_perm(centroids):\n","  permutations = []\n","  for current_permutation in itertools.permutations(centroids, 2):\n","    if current_permutation[::-1] not in permutations:\n","      permutations.append(current_permutation)\n","  return permutations"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMJYkcBZam6V"},"source":["Calculate the middle point between 2 points"]},{"cell_type":"code","metadata":{"id":"6n3hovnYsP-w"},"source":["def midpoint(p1, p2):\n","    return ((p1[0] + p2[0])/2, (p1[1] + p2[1])/2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2IcW9c8atkV"},"source":["Calculate the slope of a segment"]},{"cell_type":"code","metadata":{"id":"3jIbVOfVSIVB"},"source":["def calculate_slope(x1, y1, x2, y2):\n","    m = (y2-y1)/(x2-x1)\n","    return m"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3UrxuWyaylN"},"source":["Main function to predict pedestrians and calculate the distance between them"]},{"cell_type":"code","metadata":{"id":"7Bp4kr5Iaow4"},"source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from random import randrange\n","\n","def show_inference(image_path):\n","  image = Image.open(image_path)\n","  # the array based representation of the image will be used later in order to prepare the\n","  # result image with boxes and labels on it.\n","  image_np = load_image_into_numpy_array(image)\n","  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","  image_np_expanded = np.expand_dims(image_np, axis=0)\n","  # Actual detection.\n","  output_dict = run_inference_for_single_image(image_np, detection_graph)\n","\n","  # Get boxes only for person\n","  confidence_cutoff = 0.5\n","  boxes, scores, classes = filter_boxes(confidence_cutoff, output_dict['detection_boxes'], output_dict['detection_scores'], output_dict['detection_classes'], [1])\n","\n","  # Get width and heigth\n","  im = Image.fromarray(image_np)\n","  width, height = im.size\n","\n","  # Pixel per meters - THIS IS A REFERENCE, YOU HAVE TO ADAPT THIS FOR EACH IMAGE\n","  # In this case, we are considering that (width - 150) approximately is 7 meters\n","  average_px_meter = (width-150) / 7\n","\n","  # Calculate normalized coordinates for boxes\n","  centroids = []\n","  coordinates = []\n","  for box in boxes:\n","    coord = calculate_coord(box, width, height)\n","    centr = calculate_centr(coord)\n","    centroids.append(centr)\n","    coordinates.append(coord)\n","\n","  # Calculate all permutations\n","  permutations = calculate_perm(centroids)\n","\n","  # Display boxes and centroids\n","  fig, ax = plt.subplots(figsize = (20,12), dpi = 90)\n","  ax.imshow(image, interpolation='nearest')\n","  for coord, centr in zip(coordinates, centroids):\n","    ax.add_patch(patches.Rectangle((coord[0], coord[1]), coord[2], coord[3], linewidth=2, edgecolor='y', facecolor='none', zorder=10))\n","    ax.add_patch(patches.Circle((centr[0], centr[1]), 3, color='yellow', zorder=20))\n","\n","  # Display lines between centroids\n","  for perm in permutations:\n","    dist = calculate_centr_distances(perm[0], perm[1])\n","    dist_m = dist/average_px_meter\n","\n","    print(\"M meters: \", dist_m)\n","    middle = midpoint(perm[0], perm[1])\n","    print(\"Middle point\", middle)\n","\n","    x1 = perm[0][0]\n","    x2 = perm[1][0]\n","    y1 = perm[0][1]\n","    y2 = perm[1][1]\n","\n","    slope = calculate_slope(x1, y1, x2, y2)\n","    dy = math.sqrt(3**2/(slope**2+1))\n","    dx = -slope*dy\n","\n","    # Display randomly the position of our distance text\n","    if randrange(10) % 2== 0:\n","      Dx = middle[0] - dx*10\n","      Dy = middle[1] - dy*10\n","    else:\n","      Dx = middle[0] + dx*10\n","      Dy = middle[1] + dy*10\n","\n","    if dist_m < 1.5:\n","      ax.annotate(\"{}m\".format(round(dist_m, 2)), xy=middle, color='white', xytext=(Dx, Dy), fontsize=10, arrowprops=dict(arrowstyle='->', lw=1.5, color='yellow'), bbox=dict(facecolor='red', edgecolor='white', boxstyle='round', pad=0.2), zorder=30)\n","      ax.plot((perm[0][0], perm[1][0]), (perm[0][1], perm[1][1]), linewidth=2, color='yellow', zorder=15)\n","    elif 1.5 < dist_m < 3.5:\n","      ax.annotate(\"{}m\".format(round(dist_m, 2)), xy=middle, color='black', xytext=(Dx, Dy), fontsize=8, arrowprops=dict(arrowstyle='->', lw=1.5, color='skyblue'), bbox=dict(facecolor='y', edgecolor='white', boxstyle='round', pad=0.2), zorder=30)\n","      ax.plot((perm[0][0], perm[1][0]), (perm[0][1], perm[1][1]), linewidth=2, color='skyblue', zorder=15)\n","    else:\n","      pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxUTQeFFAXG7"},"source":["Uncomment show_inference() and add image path ==> show_inference('img.jpg')"]},{"cell_type":"code","metadata":{"id":"G1gmtmD_19AF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4eedd2a-ce7a-440f-f788-9b6e732f6c29"},"source":["# Make prediction\n","#show_inference('')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["M meters:  3.7905392563844456\n","Middle point (602.978935203515, 443.427863240242)\n","M meters:  6.065934537251108\n","Middle point (989.2125168954954, 753.6752786636353)\n","M meters:  1.1491598110480408\n","Middle point (323.3983964314684, 679.1379255056381)\n","M meters:  3.6833364974628497\n","Middle point (1418.1121798455715, 478.0501900911331)\n","M meters:  2.7188373353844293\n","Middle point (752.2980593815446, 403.512836933136)\n","M meters:  4.981214333427742\n","Middle point (1138.531641073525, 713.7602523565292)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yD01nC1MYCdB"},"source":["# Videos"]},{"cell_type":"code","metadata":{"id":"ELgbuKy-NcBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616327362523,"user_tz":-300,"elapsed":269222,"user":{"displayName":"AhmedShahid Muhammadi","photoUrl":"","userId":"11627724979041663556"}},"outputId":"f440c973-9a2b-46f8-de30-6e4ec5762fd4"},"source":["import cv2\n","import matplotlib\n","from matplotlib import pyplot as plt\n","plt.ioff()\n","matplotlib.use('Agg')\n","\n","FILE_OUTPUT = '/Projects/TEST/Predicted.mp4'\n","\n","# Playing video from file\n","cap = cv2.VideoCapture('/models/research/object_detection/test_images/WhatsApp Video 2021-03-21 at 3.56.07 PM.mp4')\n","\n","# Default resolutions of the frame are obtained.The default resolutions are system dependent.\n","# We convert the resolutions from float to integer.\n","width = int(cap.get(3))\n","height = int(cap.get(4))\n","\n","dim = (width, height)\n","print(dim)\n","\n","tope = 10\n","i = 0\n","new = True\n","with detection_graph.as_default():\n","    with tf.Session(graph=detection_graph) as sess:\n","        # Definite input and output Tensors for detection_graph\n","        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n","\n","        # Each box represents a part of the image where a particular object was detected.\n","        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n","\n","        # Each score represent how level of confidence for each of the objects.\n","        # Score is shown on the result image, together with the class label.\n","        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n","        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n","        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n","\n","        i = 0\n","        while(cap.isOpened()):\n","            # Capture frame-by-frame\n","            ret, frame = cap.read()\n","            \n","            if ret == True:\n","              # Correct color\n","              frame = gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","              # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","              image_np_expanded = np.expand_dims(frame, axis=0)\n","\n","              # Actual detection.\n","              (boxes, scores, classes, num) = sess.run(\n","                  [detection_boxes, detection_scores, detection_classes, num_detections],\n","                  feed_dict={image_tensor: image_np_expanded})\n","              \n","              # Filter boxes\n","              confidence_cutoff = 0.5\n","              boxes, scores, classes = filter_boxes(confidence_cutoff, np.squeeze(boxes), np.squeeze(scores), np.squeeze(classes), [1])\n","\n","              # Calculate normalized coordinates for boxes\n","              centroids = []\n","              coordinates = []\n","              for box in boxes:\n","                coord = calculate_coord(box, width, height)\n","                centr = calculate_centr(coord)\n","                centroids.append(centr)\n","                coordinates.append(coord)\n","\n","              # Pixel per meters\n","              average_px_meter = (width-150) / 7\n","\n","              permutations = calculate_perm(centroids)\n","\n","              # Display boxes and centroids\n","              fig, ax = plt.subplots(figsize = (20,12), dpi = 90, frameon=False)\n","              ax = fig.add_axes([0, 0, 1, 1])\n","              ax.axis('off')\n","              ax.spines['top'].set_visible(False)\n","              ax.spines['right'].set_visible(False)\n","              ax.spines['bottom'].set_visible(False)\n","              ax.spines['left'].set_visible(False)\n","              ax.get_xaxis().set_ticks([])\n","              ax.get_yaxis().set_ticks([])\n","              for coord, centr in zip(coordinates, centroids):\n","                ax.add_patch(patches.Rectangle((coord[0], coord[1]), coord[2], coord[3], linewidth=2, edgecolor='y', facecolor='none', zorder=10))\n","                ax.add_patch(patches.Circle((centr[0], centr[1]), 3, color='yellow', zorder=20))\n","\n","              # Display lines between centroids\n","              for perm in permutations:\n","                dist = calculate_centr_distances(perm[0], perm[1])\n","                dist_m = dist/average_px_meter\n","                \n","                x1 = perm[0][0]\n","                y1 = perm[0][1]\n","                x2 = perm[1][0]\n","                y2 = perm[1][1]\n","\n","                # Calculate middle point\n","                middle = midpoint(perm[0], perm[1])\n","\n","                # Calculate slope\n","                slope = calculate_slope(x1, y1, x2, y2)\n","                dy = math.sqrt(3**2/(slope**2+1))\n","                dx = -slope*dy\n","\n","                # Set random location\n","                if randrange(10) % 2== 0:\n","                  Dx = middle[0] - dx*10\n","                  Dy = middle[1] - dy*10\n","                else:\n","                  Dx = middle[0] + dx*10\n","                  Dy = middle[1] + dy*10\n","\n","                if dist_m < 1.5:\n","                  ax.annotate(\"{}m\".format(round(dist_m, 2)), xy=middle, color='white', xytext=(Dx, Dy), fontsize=10, arrowprops=dict(arrowstyle='->', lw=1.5, color='yellow'), bbox=dict(facecolor='red', edgecolor='white', boxstyle='round', pad=0.2), zorder=35)\n","                  ax.plot((perm[0][0], perm[1][0]), (perm[0][1], perm[1][1]), linewidth=2, color='yellow', zorder=15)\n","                elif 1.5 < dist_m < 3.5:\n","                  ax.annotate(\"{}m\".format(round(dist_m, 2)), xy=middle, color='black', xytext=(Dx, Dy), fontsize=8, arrowprops=dict(arrowstyle='->', lw=1.5, color='skyblue'), bbox=dict(facecolor='y', edgecolor='white', boxstyle='round', pad=0.2), zorder=35)\n","                  ax.plot((perm[0][0], perm[1][0]), (perm[0][1], perm[1][1]), linewidth=2, color='skyblue', zorder=15)\n","                else:\n","                  pass\n","              \n","              ax.imshow(frame, interpolation='nearest')\n","              \n","              # This allows you to save each frame in a folder\n","              #fig.savefig(\"/gdrive/My Drive/Projects/Pedestrian_Detection/models/research/test_images/TEST_{}.png\".format(i))\n","              # i += 1\n","\n","              # Convert figure to numpy\n","              fig.canvas.draw()\n","\n","              img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n","              img  = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n","\n","              img = np.array(fig.canvas.get_renderer()._renderer)\n","              img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n","\n","              if new:\n","                print(\"Define out\")\n","                out = cv2.VideoWriter(FILE_OUTPUT, cv2.VideoWriter_fourcc(*'MP4V'), 20.0, (img.shape[1], img.shape[0]))\n","                new = False\n","\n","              out.write(img)\n","            else:\n","              break\n","\n","    # When everything done, release the video capture and video write objects\n","    cap.release()\n","    out.release()\n","\n","    # Closes all the frames\n","    cv2.destroyAllWindows()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1920, 1080)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"],"name":"stderr"},{"output_type":"stream","text":["Define out\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3ea9whBL9hjX"},"source":[""],"execution_count":null,"outputs":[]}]}